{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras import layers\nfrom keras.models import Model\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom imgaug import augmenters as iaa\n\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:02:28.518041Z","iopub.execute_input":"2022-08-03T12:02:28.518735Z","iopub.status.idle":"2022-08-03T12:02:28.527346Z","shell.execute_reply.started":"2022-08-03T12:02:28.518698Z","shell.execute_reply":"2022-08-03T12:02:28.525947Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"x_real = np.load('../input/dataset-z/x_real.npz')['data']\ny_real = np.load('../input/dataset-z/y_real.npy')\n\nx_zoom = np.load('../input/dataset-z/x_zoom.npz')['data']\ny_zoom = np.load('../input/dataset-z/y_zoom.npy')\n\nx_partial = np.load('../input/dataset-z/x_partial.npz')['data']\ny_partial = np.load('../input/dataset-z/y_partial.npy')\n\nprint(x_zoom.shape, y_zoom.shape)\nprint(x_partial.shape, y_partial.shape)\n\nplt.figure(figsize=(15, 10))\nplt.subplot(1, 3, 1)\nplt.title(y_real[0])\nplt.imshow(x_real[0].squeeze(), cmap='gray')\nplt.subplot(1, 3, 2)\nplt.title(y_zoom[0])\nplt.imshow(x_zoom[0].squeeze(), cmap='gray')\nplt.subplot(1, 3, 3)\nplt.title(y_partial[0])\nplt.imshow(x_partial[0].squeeze(), cmap='gray')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T12:02:28.553328Z","iopub.execute_input":"2022-08-03T12:02:28.553639Z","iopub.status.idle":"2022-08-03T12:02:29.333156Z","shell.execute_reply.started":"2022-08-03T12:02:28.553610Z","shell.execute_reply":"2022-08-03T12:02:29.332238Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"x_train, x_val, label_train, label_val = train_test_split(x_zoom, y_zoom, test_size=0.1)\n\nprint(x_zoom.shape, y_zoom.shape)\nprint(x_train.shape, label_train.shape)\nprint(x_val.shape, label_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:02:29.335109Z","iopub.execute_input":"2022-08-03T12:02:29.335715Z","iopub.status.idle":"2022-08-03T12:02:29.397800Z","shell.execute_reply.started":"2022-08-03T12:02:29.335674Z","shell.execute_reply":"2022-08-03T12:02:29.396797Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Make Label Dictionary Lookup Table","metadata":{}},{"cell_type":"code","source":"# ID(3)性別(1)左右(1)指頭(1): index\n# {'100001': 0, '100004': 1, '100002': 2, ....}\nlabel_real_dict = {}\n\nfor i, y in enumerate(y_real):\n    key = y.astype(str)\n    key = ''.join(key).zfill(6)\n\n    label_real_dict[key] = i\nlen(label_real_dict)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:02:29.399412Z","iopub.execute_input":"2022-08-03T12:02:29.399780Z","iopub.status.idle":"2022-08-03T12:02:29.441530Z","shell.execute_reply.started":"2022-08-03T12:02:29.399742Z","shell.execute_reply":"2022-08-03T12:02:29.440495Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Data Generator","metadata":{}},{"cell_type":"code","source":"class DataGenerator(keras.utils.all_utils.Sequence):\n    def __init__(self, x, label, x_real, label_real_dict, batch_size=32, shuffle=True):\n        'Initialization'\n        self.x = x\n        self.label = label\n        self.x_real = x_real\n        self.label_real_dict = label_real_dict\n        \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.x) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        x1_batch = self.x[index*self.batch_size:(index+1)*self.batch_size]\n        label_batch = self.label[index*self.batch_size:(index+1)*self.batch_size]\n        \n        x2_batch = np.empty((self.batch_size, 96, 96), dtype=np.float32)\n        y_batch = np.zeros((self.batch_size, 1), dtype=np.float32)\n        \n        # augmentation\n        if self.shuffle:\n            seq = iaa.Sequential([\n                iaa.GaussianBlur(sigma=(0, 0.5)),\n                iaa.Affine(\n                    scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                    translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n                    rotate=(-30, 30),\n                    order=[0, 1],\n                    cval=255\n                )\n            ], random_order=True)\n\n            x1_batch = seq.augment_images(x1_batch)\n        \n        # pick matched images(label 1.0) and unmatched images(label 0.0) and put together in batch\n        # matched images must be all same, [subject_id(3), gender(1), left_right(1), finger(1)], e.g) 034010\n        for i, l in enumerate(label_batch):\n            match_key = l.astype(str)\n            match_key = ''.join(match_key).zfill(6)\n\n            if random.random() > 0.5:\n                # put matched image\n                x2_batch[i] = self.x_real[self.label_real_dict[match_key]]\n                y_batch[i] = 1.\n            else:\n                # put unmatched image\n                while True:\n                    unmatch_key, unmatch_idx = random.choice(list(self.label_real_dict.items()))\n\n                    if unmatch_key != match_key:\n                        break\n\n                x2_batch[i] = self.x_real[unmatch_idx]\n                y_batch[i] = 0.\n        \n        return [x1_batch.astype(np.float32) / 255., x2_batch.astype(np.float32) / 255.], y_batch\n\n    def on_epoch_end(self):\n        if self.shuffle == True:\n            self.x, self.label = shuffle(self.x, self.label)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:02:29.444346Z","iopub.execute_input":"2022-08-03T12:02:29.444844Z","iopub.status.idle":"2022-08-03T12:02:29.459067Z","shell.execute_reply.started":"2022-08-03T12:02:29.444807Z","shell.execute_reply":"2022-08-03T12:02:29.457853Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_gen = DataGenerator(x_train, label_train, x_real, label_real_dict, shuffle=True)\nval_gen = DataGenerator(x_val, label_val, x_real, label_real_dict, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:02:29.460648Z","iopub.execute_input":"2022-08-03T12:02:29.461064Z","iopub.status.idle":"2022-08-03T12:02:29.518588Z","shell.execute_reply.started":"2022-08-03T12:02:29.461030Z","shell.execute_reply":"2022-08-03T12:02:29.517605Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"x1 = layers.Input(shape=(96, 96, 1))\nx2 = layers.Input(shape=(96, 96, 1))\n\n# share weights both inputs\ninputs = layers.Input(shape=(96, 96, 1))\n\nfeature = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(inputs)\nfeature = layers.MaxPooling2D(pool_size=2)(feature)\n\nfeature = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(feature)\nfeature = layers.MaxPooling2D(pool_size=2)(feature)\n\nfeature = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(feature)\nfeature = layers.MaxPooling2D(pool_size=2)(feature)\n\nfeature = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(feature)\nfeature = layers.MaxPooling2D(pool_size=2)(feature)\n\nfeature_model = Model(inputs=inputs, outputs=feature)\n\n# 2 feature models that sharing weights\nx1_net = feature_model(x1)\nx2_net = feature_model(x2)\n\n# subtract features\nnet = layers.Subtract()([x1_net, x2_net])\nnet = layers.Conv2D(32, kernel_size=3, padding='same', activation='relu')(net)\nnet = layers.MaxPooling2D(pool_size=2)(net)\nnet = layers.Flatten()(net)\nnet = layers.Dense(64, activation='relu')(net)\nnet = layers.Dense(1, activation='sigmoid')(net)\n\nmodel = Model(inputs=[x1, x2], outputs=net)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:02:29.520802Z","iopub.execute_input":"2022-08-03T12:02:29.521409Z","iopub.status.idle":"2022-08-03T12:02:29.637266Z","shell.execute_reply.started":"2022-08-03T12:02:29.521370Z","shell.execute_reply":"2022-08-03T12:02:29.636276Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n# 建立 EarlyStopping 物件\nes = EarlyStopping(monitor='val_loss', mode='min',\n                  verbose=1, patience=5)\n\nfrom keras.callbacks import ModelCheckpoint\n# 建立 ModelCheckpoint 物件\nfilename = './data/Siamese_zoom.h5'\n# filename = './data/Siamese_zoom.hdf5' # val_accuracy\nmc = ModelCheckpoint(filename, monitor='val_acc',\n                    mode='max', verbose=0,\n                    save_best_only=True)\n\nhistory = model.fit(train_gen, epochs=100, validation_data=val_gen, callbacks=[es, mc])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-03T12:02:29.638905Z","iopub.execute_input":"2022-08-03T12:02:29.639278Z","iopub.status.idle":"2022-08-03T12:17:13.670154Z","shell.execute_reply.started":"2022-08-03T12:02:29.639235Z","shell.execute_reply":"2022-08-03T12:17:13.669095Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# save model","metadata":{}},{"cell_type":"code","source":"# # 儲存Keras模型\n# print('Saving Model: Siamese_zoom.h5 ...')\n# model.save('./data/Siamese_zoom.h5')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:17:13.672110Z","iopub.execute_input":"2022-08-03T12:17:13.672477Z","iopub.status.idle":"2022-08-03T12:17:13.678036Z","shell.execute_reply.started":"2022-08-03T12:17:13.672440Z","shell.execute_reply":"2022-08-03T12:17:13.675919Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# load model","metadata":{}},{"cell_type":"code","source":"# model =  keras.models.load_model('./data/Siamese_zoom.h5')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:17:13.680214Z","iopub.execute_input":"2022-08-03T12:17:13.681075Z","iopub.status.idle":"2022-08-03T12:17:13.689331Z","shell.execute_reply.started":"2022-08-03T12:17:13.681035Z","shell.execute_reply":"2022-08-03T12:17:13.688134Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"match = np.ones((6000,1))\nmatch.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:17:13.694207Z","iopub.execute_input":"2022-08-03T12:17:13.695364Z","iopub.status.idle":"2022-08-03T12:17:13.704624Z","shell.execute_reply.started":"2022-08-03T12:17:13.695329Z","shell.execute_reply":"2022-08-03T12:17:13.703383Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# 評估模型\nprint('\\nTesting ...')\nloss, accuracy = model.evaluate([x_partial.astype(np.float32) / 255.,x_real.astype(np.float32) / 255.], match, verbose=1)\nprint('測試資料集的準確度 = {:.2f}'.format(accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:17:13.706204Z","iopub.execute_input":"2022-08-03T12:17:13.706674Z","iopub.status.idle":"2022-08-03T12:17:15.848178Z","shell.execute_reply.started":"2022-08-03T12:17:13.706639Z","shell.execute_reply":"2022-08-03T12:17:15.847239Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# 顯示訓練和驗證損失\nloss = history.history['loss']\nepochs = range(1, len(loss) + 1)\nval_loss = history.history['val_loss']\nplt.plot(epochs, loss, 'bo-', label='Training Loss')\nplt.plot(epochs, val_loss, 'ro--', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n# 顯示訓練和驗證準確度 注意 accyracy 要改成 acc，val_accuracy => val_acc，因為keras版本問題\nacc = history.history['acc']\nepochs = range(1, len(acc) + 1)\nval_acc = history.history['val_acc']\nplt.plot(epochs, acc, 'bo-', label='Training Acc')\nplt.plot(epochs, val_acc, 'ro--', label='Validation Acc')\nplt.title('Training and Validation Acc')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:17:15.850999Z","iopub.execute_input":"2022-08-03T12:17:15.851287Z","iopub.status.idle":"2022-08-03T12:17:16.249776Z","shell.execute_reply.started":"2022-08-03T12:17:15.851259Z","shell.execute_reply":"2022-08-03T12:17:16.248828Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# new user fingerprint input\nrandom_idx = random.randint(0, len(x_partial))\n\nrandom_img = x_partial[random_idx]\nrandom_label = y_partial[random_idx]\n\nrandom_img = random_img.reshape((1, 96, 96, 1)).astype(np.float32) / 255.\n\n# matched image\nmatch_key = random_label.astype(str)\nmatch_key = ''.join(match_key).zfill(6)\n\nrx = x_real[label_real_dict[match_key]].reshape((1, 96, 96, 1)).astype(np.float32) / 255.\nry = y_real[label_real_dict[match_key]]\n\npred_rx = model.predict([random_img, rx])\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1)\nplt.title('Input: %s' %random_label)\nplt.imshow(random_img.squeeze(), cmap='gray')\nplt.subplot(1, 2, 2)\nplt.title('Real: %.02f, %s' % (pred_rx, ry))\nplt.imshow(rx.squeeze(), cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-08-03T12:17:16.251035Z","iopub.execute_input":"2022-08-03T12:17:16.251584Z","iopub.status.idle":"2022-08-03T12:17:16.677739Z","shell.execute_reply.started":"2022-08-03T12:17:16.251552Z","shell.execute_reply":"2022-08-03T12:17:16.676798Z"},"trusted":true},"execution_count":28,"outputs":[]}]}